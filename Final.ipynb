{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e742853c",
      "metadata": {
        "id": "e742853c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, models\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "23b17c8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23b17c8d",
        "outputId": "dc0fb837-8ccd-45eb-b261-ebfac4a94e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1c687b9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1c687b9f",
        "outputId": "24316932-f1de-408b-8551-e8ed45ecba95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Directory contents: ['.config', 'drive', 'sample_data']\n",
            "Classes in directory: ['LOADED', 'UNLOADED']\n",
            "Number of images in LOADED: 51\n",
            "Number of images in UNLOADED: 0\n",
            "Found 41 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Number of training samples: 41\n",
            "Number of validation samples: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m44,302,848\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">44,302,848</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,396,609\u001b[0m (169.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,396,609</span> (169.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,396,609\u001b[0m (169.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,396,609</span> (169.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15s/step - accuracy: 0.8934 - loss: 0.3314 - val_accuracy: 1.0000 - val_loss: 3.3621e-16\n",
            "Epoch 2/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 897ms/step - accuracy: 1.0000 - loss: 4.7281e-16 - val_accuracy: 1.0000 - val_loss: 4.4860e-33\n",
            "Epoch 3/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 7.7058e-28 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 690ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 684ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893ms/step\n",
            "Prediction for /content/drive/MyDrive/PhotosOfLoaded/DATASET/LOADED/1.jpg: UnLoaded\n",
            "\n",
            "Final Training Metrics:\n",
            "Training Accuracy: 1.0000\n",
            "Training Loss: 0.0000\n",
            "Validation Accuracy: 1.0000\n",
            "Validation Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import os\n",
        "\n",
        "# Debug prints to check directory and contents\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Directory contents:\", os.listdir())\n",
        "\n",
        "# Set image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Set up data generators with image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Specify the dataset directory\n",
        "dataset_dir = '/content/drive/MyDrive/PhotosOfLoaded/DATASET'  # Assuming the damaged and no_damage folders are in the current directory\n",
        "\n",
        "\n",
        "if not os.path.exists(dataset_dir):\n",
        "    raise ValueError(f\"Dataset directory not found: {dataset_dir}\")\n",
        "\n",
        "\n",
        "# Debug prints for directory structure\n",
        "print(\"Classes in directory:\", os.listdir(dataset_dir))\n",
        "\n",
        "# Count images in each folder\n",
        "for folder in ['LOADED', 'UNLOADED']:\n",
        "    folder_path = os.path.join(dataset_dir, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        print(f\"Number of images in {folder}: {len(files)}\")\n",
        "    else:\n",
        "        print(f\"Folder {folder} does not exist!\")\n",
        "\n",
        "# Create train and validation generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    classes=['LOADED', 'UNLOADED']\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    classes=['LOADED', 'UNLOADED']\n",
        ")\n",
        "\n",
        "# Print the number of samples\n",
        "print(\"Number of training samples:\", train_generator.samples)\n",
        "print(\"Number of validation samples:\", validation_generator.samples)\n",
        "\n",
        "# Create the CNN model\n",
        "model = Sequential([\n",
        "    # First Convolutional Block\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Flatten and Dense Layers\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('damage_detection_model.h5')\n",
        "\n",
        "# Function to predict a single image\n",
        "def predict_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(\n",
        "        image_path,\n",
        "        target_size=(img_width, img_height)\n",
        "    )\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    img_array /= 255.\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    return \"Loaded\" if prediction[0] > 0.5 else \"UnLoaded\"\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Example of how to use the prediction function\n",
        "# Replace 'path_to_test_image.jpg' with an actual image path\n",
        "try:\n",
        "    test_image_path = '/content/drive/MyDrive/PhotosOfLoaded/DATASET/LOADED/1.jpg'  # Replace with your test image path\n",
        "    result = predict_image(test_image_path)\n",
        "    print(f\"Prediction for {test_image_path}: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during prediction: {str(e)}\")\n",
        "\n",
        "# Print final metrics\n",
        "final_train_accuracy = history.history['accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_accuracy = history.history['val_accuracy'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(\"\\nFinal Training Metrics:\")\n",
        "print(f\"Training Accuracy: {final_train_accuracy:.4f}\")\n",
        "print(f\"Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {final_val_accuracy:.4f}\")\n",
        "print(f\"Validation Loss: {final_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b096908f",
      "metadata": {
        "id": "b096908f"
      },
      "outputs": [],
      "source": [
        "# Function to predict a single image\n",
        "def predict_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(\n",
        "        image_path,\n",
        "        target_size=(img_width, img_height)\n",
        "    )\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    img_array /= 255.\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    return \"Un Loaded\" if prediction[0] > 0.5 else \"Loaded\"\n",
        "\n",
        "try:\n",
        "    test_image_path = '/content/drive/MyDrive/PhotosOfLoaded/DATASET/LOADED/1.jpg'  # Replace with your test image path\n",
        "    result = predict_image(test_image_path)\n",
        "    print(f\"Prediction for {test_image_path}: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during prediction: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec214de",
      "metadata": {
        "id": "2ec214de"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b28267",
      "metadata": {
        "id": "28b28267"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "import cv2\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2 import model_zoo\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Data and model paths (adjust if needed)\n",
        "data_dir = \"/content/drive/MyDrive/Wagon edges detection.v1i.coco\"\n",
        "output_dir = \"/content/drive/MyDrive/Wagon_count_output\"\n",
        "for d in [\"train\",\"test\"]:\n",
        "    json_file = os.path.join(data_dir, d, \"_annotations.coco.json\")\n",
        "    image_dir = os.path.join(data_dir, d)\n",
        "    register_coco_instances(f\"my_dataset_{d}\", {}, json_file, image_dir)\n",
        "\n",
        "# Metadata\n",
        "metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "# metadata.thing_classes = [\"objects\", \"Damage\", \"Damege\", \"Debris\", \"Obstacle\"]\n",
        "# metadata.thing_dataset_id_to_contiguous_id = {0:0, 1:1, 2:2, 3:3, 4:4}\n",
        "metadata.thing_classes = [\"Rail-Wagon\", \"Front\", \"Rear\"]\n",
        "metadata.thing_dataset_id_to_contiguous_id = {0: 0, 1: 1, 2: 2}\n",
        "\n",
        "\n",
        "# Detectron2 Configuration\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "# OR, if you have a local config file:\n",
        "#cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.VAL = ()\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\",)  # Include the test set\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2  # Adjust based on GPU\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000  # Adjust as needed\n",
        "cfg.OUTPUT_DIR = output_dir\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ... (other code)\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# *** REPLACE THIS WITH THE ACTUAL PATH TO YOUR TEST IMAGE ***\n",
        "image_path = \"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_1250_jpg.rf.d06658a856f4f86b79c7fdceb4cd7566.jpg\"\n",
        "# OR\n",
        "# image_path = os.path.join(data_dir, \"test\", \"frame_1320_jpg.rf.4aee7ba0b6ca9bc34b30f0dcc2559dcf.jpg\")  # Example - CHANGE THIS!\n",
        "\n",
        "print(f\"Trying to read image at: {image_path}\") # Print the path for debugging\n",
        "im = cv2.imread(image_path)\n",
        "\n",
        "if im is None:\n",
        "    print(f\"ERROR: Could not read image at: {image_path}\")  # Handle the error\n",
        "else:\n",
        "    outputs = predictor(im)\n",
        "    instances = outputs[\"instances\"].to(torch.device(\"cpu\"))\n",
        "    high_conf_instances = instances[instances.scores > 0.8]\n",
        "\n",
        "    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "    out = v.draw_instance_predictions(high_conf_instances)\n",
        "\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506e9b16",
      "metadata": {
        "id": "506e9b16"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # Load your trained model\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "paths=[\"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_1250_jpg.rf.d06658a856f4f86b79c7fdceb4cd7566.jpg\",\"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_1480_jpg.rf.f2059211ffa29fac69ce6d52934dbe9d.jpg\",\"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_1970_jpg.rf.473aed8e1ed837640b689179aed9eb75.jpg\",\"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_550_jpg.rf.e9a4990d405cfc21dfa87a3cdafaf424.jpg\"]\n",
        "\n",
        "\n",
        "for i in range(len(paths)):\n",
        "    frame = cv2.imread(paths[i])\n",
        "\n",
        "\n",
        "    outputs = predictor(frame)\n",
        "\n",
        "    # Filter by confidence threshold\n",
        "    instances = outputs[\"instances\"].to(torch.device(\"cpu\"))\n",
        "    high_conf_instances = instances[instances.scores > 0.8]\n",
        "\n",
        "    v = Visualizer(frame[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "    out = v.draw_instance_predictions(high_conf_instances)\n",
        "\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])  # Display the frame in Colab\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
        "        break\n",
        "\n",
        "\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18665be9-800e-4440-86d3-a07d4cdaad90",
      "metadata": {
        "id": "18665be9-800e-4440-86d3-a07d4cdaad90"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ebf5cd0-ec03-40bb-a21a-9b3ec3e8fa63",
      "metadata": {
        "id": "5ebf5cd0-ec03-40bb-a21a-9b3ec3e8fa63"
      },
      "outputs": [],
      "source": [
        "# Import additional required libraries\n",
        "import tensorflow as tf\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import utils\n",
        "from mrcnn.config import Config\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Define Mask R-CNN Configuration\n",
        "class CustomConfig(Config):\n",
        "    NAME = \"custom\"\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1  # Background + your class\n",
        "\n",
        "    # Number of training steps per epoch\n",
        "    STEPS_PER_EPOCH = 100\n",
        "\n",
        "    # Learning rate\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    # Skip detections with < 90% confidence\n",
        "    DETECTION_MIN_CONFIDENCE = 0.9\n",
        "\n",
        "    # Setting batch size\n",
        "    IMAGES_PER_GPU = 1\n",
        "    GPU_COUNT = 1\n",
        "\n",
        "# Function to prepare dataset\n",
        "def prepare_dataset(df, BASE_PATH, target_size=(1024, 1024)):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Load image\n",
        "        img_path = os.path.join(BASE_PATH, str(row['filename']))\n",
        "        if os.path.exists(img_path):\n",
        "            # Read and preprocess image\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = cv2.resize(image, target_size)\n",
        "\n",
        "            # Create mask (you'll need to modify this based on your annotations)\n",
        "            mask = np.zeros(target_size + (1,), dtype=np.bool)\n",
        "            # Add mask creation logic here based on your annotations\n",
        "\n",
        "            images.append(image)\n",
        "            masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Training function\n",
        "def train_model(train_images, train_masks, val_images, val_masks, config):\n",
        "    # Initialize model\n",
        "    model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                             model_dir=\"./logs/\")\n",
        "\n",
        "    # Load pre-trained COCO weights\n",
        "    model.load_weights('mask_rcnn_coco.h5', by_name=True,\n",
        "                      exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n",
        "    # Train the model\n",
        "    model.train(train_images, train_masks,\n",
        "                val_images, val_masks,\n",
        "                learning_rate=config.LEARNING_RATE,\n",
        "                epochs=30,\n",
        "                layers='heads')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Prediction function\n",
        "def predict_image(image_path, model, config):\n",
        "    # Read and preprocess image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detect objects\n",
        "    results = model.detect([image], verbose=1)\n",
        "    r = results[0]\n",
        "\n",
        "    return r\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize configuration\n",
        "    config = CustomConfig()\n",
        "    config.display()\n",
        "\n",
        "    # Read your dataset\n",
        "    df = pd.read_csv('/Users/bhavya/Desktop/allnewphotos/image_labels.csv')\n",
        "    BASE_PATH = \"/Users/bhavya/Desktop/allnewphotos/\"\n",
        "\n",
        "    # Prepare dataset\n",
        "    images, masks = prepare_dataset(df, BASE_PATH)\n",
        "\n",
        "    # Split dataset\n",
        "    train_idx, val_idx = train_test_split(range(len(images)), test_size=0.2)\n",
        "    train_images, train_masks = images[train_idx], masks[train_idx]\n",
        "    val_images, val_masks = images[val_idx], masks[val_idx]\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(train_images, train_masks, val_images, val_masks, config)\n",
        "\n",
        "    # Test predictions\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i, img_path in enumerate(df['filename'].head(5)):\n",
        "        full_path = os.path.join(BASE_PATH, img_path)\n",
        "        results = predict_image(full_path, model, config)\n",
        "\n",
        "        # Display results\n",
        "        image = cv2.imread(full_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f'Detection {i+1}')\n",
        "\n",
        "        # Draw masks\n",
        "        for j in range(len(results['masks'])):\n",
        "            mask = results['masks'][:, :, j]\n",
        "            plt.imshow(mask, alpha=0.5)\n",
        "\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63158c3-d32c-47c1-a124-d26f24bdda6b",
      "metadata": {
        "id": "f63158c3-d32c-47c1-a124-d26f24bdda6b"
      },
      "outputs": [],
      "source": [
        "!conda install -c conda-forge keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1997366b-f80e-4ab6-8fe5-d2398f9b9219",
      "metadata": {
        "id": "1997366b-f80e-4ab6-8fe5-d2398f9b9219"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import os\n",
        "\n",
        "# Debug prints to check directory and contents\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Directory contents:\", os.listdir())\n",
        "\n",
        "# Set image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Set up data generators with image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Specify the dataset directory\n",
        "dataset_dir = '/Users/bhavya/Desktop/Wagonframes'  # Assuming the damaged and no_damage folders are in the current directory\n",
        "\n",
        "# Debug prints for directory structure\n",
        "print(\"Classes in directory:\", os.listdir(dataset_dir))\n",
        "\n",
        "# Count images in each folder\n",
        "for folder in ['damaged', 'no_damage']:\n",
        "    folder_path = os.path.join(dataset_dir, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        print(f\"Number of images in {folder}: {len(files)}\")\n",
        "    else:\n",
        "        print(f\"Folder {folder} does not exist!\")\n",
        "\n",
        "# Create train and validation generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    classes=['damaged', 'no_damage']\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    classes=['damaged', 'no_damage']\n",
        ")\n",
        "\n",
        "# Print the number of samples\n",
        "print(\"Number of training samples:\", train_generator.samples)\n",
        "print(\"Number of validation samples:\", validation_generator.samples)\n",
        "\n",
        "# Create the CNN model\n",
        "model = Sequential([\n",
        "    # First Convolutional Block\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Flatten and Dense Layers\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('damage_detection_model.h5')\n",
        "\n",
        "# Function to predict a single image\n",
        "def predict_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(\n",
        "        image_path,\n",
        "        target_size=(img_width, img_height)\n",
        "    )\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    img_array /= 255.\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    return \"Damaged\" if prediction[0] > 0.5 else \"No Damage\"\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Example of how to use the prediction function\n",
        "# Replace 'path_to_test_image.jpg' with an actual image path\n",
        "try:\n",
        "    test_image_path = '/Users/bhavya/Desktop/testing.png'  # Replace with your test image path\n",
        "    result = predict_image(test_image_path)\n",
        "    print(f\"Prediction for {test_image_path}: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during prediction: {str(e)}\")\n",
        "\n",
        "# Print final metrics\n",
        "final_train_accuracy = history.history['accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_accuracy = history.history['val_accuracy'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(\"\\nFinal Training Metrics:\")\n",
        "print(f\"Training Accuracy: {final_train_accuracy:.4f}\")\n",
        "print(f\"Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {final_val_accuracy:.4f}\")\n",
        "print(f\"Validation Loss: {final_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e5291a-75b8-461b-8088-c9fff4c81db2",
      "metadata": {
        "id": "56e5291a-75b8-461b-8088-c9fff4c81db2"
      },
      "outputs": [],
      "source": [
        "result = predict_image('/Users/bhavya/Desktop/testing.png')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66abd511-b71a-477a-8199-d87c50c71609",
      "metadata": {
        "id": "66abd511-b71a-477a-8199-d87c50c71609"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import time\n",
        "import logging\n",
        "\n",
        "class WagonDetector:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize background subtractor\n",
        "        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(\n",
        "            history=self.config.history,\n",
        "            varThreshold=self.config.var_threshold,\n",
        "            detectShadows=self.config.detect_shadows\n",
        "        )\n",
        "\n",
        "        # Initialize counters and trackers\n",
        "        self.wagon_count = 0\n",
        "        self.crossed_objects = set()\n",
        "        self.object_tracker = deque(maxlen=self.config.tracker_length)\n",
        "        self.last_count_frame = 0\n",
        "\n",
        "        # Initialize ROI\n",
        "        self.roi_points = None\n",
        "        self.roi_mask = None\n",
        "\n",
        "        # Setup logging\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        self.logger = logging.getLogger('WagonDetector')\n",
        "\n",
        "    def setup_roi(self, frame):\n",
        "        \"\"\"Initialize ROI based on frame dimensions with offset\"\"\"\n",
        "        height, width = frame.shape[:2]\n",
        "\n",
        "        # Define offset\n",
        "        x_offset = 400  # pixels from left\n",
        "        y_offset = 400  # pixels from top\n",
        "\n",
        "        # Define ROI points (trapezoid shape) with offset\n",
        "        self.roi_points = np.array([\n",
        "            [int(width * 0.1) + x_offset, int(height * 0.4) + y_offset],  # Top left\n",
        "            [int(width * 0.9) + x_offset, int(height * 0.4) + y_offset],  # Top right\n",
        "            [int(width * 0.95) + x_offset, int(height * 0.8) + y_offset], # Bottom right\n",
        "            [int(width * 0.05) + x_offset, int(height * 0.8) + y_offset]  # Bottom left\n",
        "        ], np.int32)\n",
        "\n",
        "        # Ensure ROI points are within frame boundaries\n",
        "        self.roi_points = np.clip(self.roi_points, [0, 0], [width-1, height-1])\n",
        "\n",
        "        # Create ROI mask\n",
        "        self.roi_mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
        "        cv2.fillPoly(self.roi_mask, [self.roi_points], 255)\n",
        "\n",
        "    def preprocess_frame(self, frame):\n",
        "        \"\"\"Preprocess frame for detection\"\"\"\n",
        "        # Resize frame\n",
        "        frame = cv2.resize(frame, (self.config.resize_width, self.config.resize_height))\n",
        "\n",
        "        # Initialize ROI if not already done\n",
        "        if self.roi_mask is None:\n",
        "            self.setup_roi(frame)\n",
        "\n",
        "        # Apply ROI mask\n",
        "        roi_frame = cv2.bitwise_and(frame, frame, mask=self.roi_mask)\n",
        "\n",
        "        # Apply Gaussian blur\n",
        "        blurred = cv2.GaussianBlur(roi_frame,\n",
        "                                  (self.config.blur_kernel_size, self.config.blur_kernel_size),\n",
        "                                  0)\n",
        "\n",
        "        return blurred\n",
        "\n",
        "    def detect_motion(self, frame):\n",
        "        \"\"\"Detect motion using background subtraction\"\"\"\n",
        "        # Apply background subtraction\n",
        "        fg_mask = self.bg_subtractor.apply(frame)\n",
        "\n",
        "        # Apply morphological operations\n",
        "        kernel = np.ones((5,5), np.uint8)\n",
        "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
        "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "        return fg_mask\n",
        "\n",
        "    def process_frame(self, frame, frame_count):\n",
        "        \"\"\"Process a single frame\"\"\"\n",
        "        # Preprocess frame\n",
        "        processed = self.preprocess_frame(frame)\n",
        "        height, width = processed.shape[:2]\n",
        "\n",
        "        # Detect motion\n",
        "        motion_mask = self.detect_motion(processed)\n",
        "\n",
        "        # Find contours\n",
        "        contours, _ = cv2.findContours(\n",
        "            motion_mask,\n",
        "            cv2.RETR_EXTERNAL,\n",
        "            cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        # Draw ROI\n",
        "        cv2.polylines(frame, [self.roi_points], True, (0, 255, 255), 2)\n",
        "\n",
        "        # Draw counting line with offset\n",
        "        x_offset = 400\n",
        "        y_offset = 400\n",
        "        line_y = int(height * self.config.line_position) + y_offset\n",
        "        cv2.line(frame,\n",
        "                 (int(width * 0.1) + x_offset, line_y),\n",
        "                 (int(width * 0.9) + x_offset, line_y),\n",
        "                 (0, 255, 0), 2)\n",
        "\n",
        "        # Process contours\n",
        "        for contour in contours:\n",
        "            area = cv2.contourArea(contour)\n",
        "\n",
        "            if area > self.config.min_area:\n",
        "                # Get contour centroid\n",
        "                M = cv2.moments(contour)\n",
        "                if M[\"m00\"] != 0:\n",
        "                    cx = int(M[\"m10\"] / M[\"m00\"])\n",
        "                    cy = int(M[\"m01\"] / M[\"m00\"])\n",
        "\n",
        "                    # Check if centroid is in ROI\n",
        "                    if cv2.pointPolygonTest(self.roi_points, (cx, cy), False) >= 0:\n",
        "                        # Draw centroid\n",
        "                        cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)\n",
        "\n",
        "                        # Check if object crosses the counting line\n",
        "                        if (abs(cy - line_y) < self.config.line_threshold and\n",
        "                            frame_count - self.last_count_frame > self.config.min_frames_between_counts):\n",
        "\n",
        "                            object_id = f\"{cx}_{cy}\"\n",
        "                            if object_id not in self.crossed_objects:\n",
        "                                self.wagon_count += 1\n",
        "                                self.crossed_objects.add(object_id)\n",
        "                                self.last_count_frame = frame_count\n",
        "                                self.logger.info(f\"Wagon detected! Count: {self.wagon_count}\")\n",
        "\n",
        "                        # Draw bounding rectangle\n",
        "                        x, y, w, h = cv2.boundingRect(contour)\n",
        "                        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # Add text overlay\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            f\"Wagon Count: {self.wagon_count}\",\n",
        "            (10, 30),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            1,\n",
        "            (0, 255, 0),\n",
        "            2\n",
        "        )\n",
        "\n",
        "        return frame\n",
        "\n",
        "class WagonDetectorConfig:\n",
        "    def __init__(self):\n",
        "        # Video processing parameters\n",
        "        self.resize_width = 1920  # Increased for better resolution\n",
        "        self.resize_height = 1080\n",
        "        self.blur_kernel_size = 21\n",
        "\n",
        "        # Background subtractor parameters\n",
        "        self.history = 500\n",
        "        self.var_threshold = 16\n",
        "        self.detect_shadows = False\n",
        "\n",
        "        # Detection parameters\n",
        "        self.min_area = 3000\n",
        "        self.line_position = 0.4  # Adjusted for offset\n",
        "        self.line_threshold = 10\n",
        "        self.min_frames_between_counts = 15\n",
        "        self.tracker_length = 50\n",
        "\n",
        "def process_video(video_path, output_path=None, display=True):\n",
        "    # Create configuration\n",
        "    config = WagonDetectorConfig()\n",
        "\n",
        "    # Initialize detector\n",
        "    detector = WagonDetector(config)\n",
        "\n",
        "    # Open video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(\"Error opening video file\")\n",
        "\n",
        "    # Get video properties\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Initialize video writer if output path is provided\n",
        "    if output_path:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    processing_times = []\n",
        "\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Process frame\n",
        "            processed_frame = detector.process_frame(frame, frame_count)\n",
        "\n",
        "            # Calculate processing time\n",
        "            processing_time = time.time() - start_time\n",
        "            processing_times.append(processing_time)\n",
        "\n",
        "            # Write frame if output path is provided\n",
        "            if output_path:\n",
        "                out.write(processed_frame)\n",
        "\n",
        "            # Display frame\n",
        "            if display:\n",
        "                cv2.imshow('Wagon Detection', processed_frame)\n",
        "\n",
        "                # Break loop if 'q' is pressed\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing video: {str(e)}\")\n",
        "\n",
        "    finally:\n",
        "        # Calculate and print performance metrics\n",
        "        if processing_times:\n",
        "            avg_processing_time = np.mean(processing_times)\n",
        "            avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n",
        "\n",
        "            print(f\"\\nPerformance Metrics:\")\n",
        "            print(f\"Average processing time per frame: {avg_processing_time:.3f} seconds\")\n",
        "            print(f\"Average FPS: {avg_fps:.2f}\")\n",
        "\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        if output_path:\n",
        "            out.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    return detector.wagon_count\n",
        "\n",
        "def main():\n",
        "    # Example usage with specific video dimensions\n",
        "    video_path = \"3.mp4\"  # Replace with your video path\n",
        "    output_path = \"output_video.mp4\"     # Optional output path\n",
        "\n",
        "    try:\n",
        "        print(\"Processing video...\")\n",
        "        wagon_count = process_video(video_path, output_path)\n",
        "        print(f\"\\nTotal wagons detected: {wagon_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dbc93e5",
      "metadata": {
        "collapsed": true,
        "id": "8dbc93e5",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install pycocotools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18610902",
      "metadata": {
        "id": "18610902"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd02667",
      "metadata": {
        "id": "bcd02667"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/My Drive/Wagon frames/train copy\"\n",
        "coco_json = f\"{dataset_path}/_annotations.coco.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0c6a70",
      "metadata": {
        "collapsed": true,
        "id": "2e0c6a70",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Register dataset\n",
        "register_coco_instances(\"wagon_train\", {}, coco_json, dataset_path)\n",
        "\n",
        "# Verify dataset\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "metadata = MetadataCatalog.get(\"wagon_train\")\n",
        "dataset_dicts = DatasetCatalog.get(\"wagon_train\")\n",
        "\n",
        "# Show sample images\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    img_path = d[\"file_name\"]\n",
        "    img = cv2.imread(img_path)\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "    out = v.draw_dataset_dict(d)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(out.get_image())\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89469169",
      "metadata": {
        "id": "89469169"
      },
      "outputs": [],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f97e9fc",
      "metadata": {
        "id": "4f97e9fc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "import cv2\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2 import model_zoo\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Data and model paths (adjust if needed)\n",
        "data_dir = \"/content/drive/MyDrive/My First Project.v1i.coco-segmentation\"\n",
        "output_dir = \"/content/drive/MyDrive/detectron2_output\"\n",
        "\n",
        "# Dataset Registration (Now includes \"test\")\n",
        "for d in [\"train\", \"valid\", \"test\"]:\n",
        "    json_file = os.path.join(data_dir, d, \"_annotations.coco.json\")\n",
        "    image_dir = os.path.join(data_dir, d)\n",
        "    register_coco_instances(f\"my_dataset_{d}\", {}, json_file, image_dir)\n",
        "\n",
        "# Metadata\n",
        "metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "metadata.thing_classes = [\"objects\", \"Damage\", \"Damege\", \"Debris\", \"Obstacle\"]\n",
        "metadata.thing_dataset_id_to_contiguous_id = {0:0, 1:1, 2:2, 3:3, 4:4}\n",
        "\n",
        "# Detectron2 Configuration\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "# OR, if you have a local config file:\n",
        "#cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.VAL = (\"my_dataset_valid\",)\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\",)  # Include the test set\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2  # Adjust based on GPU\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000  # Adjust as needed\n",
        "cfg.OUTPUT_DIR = output_dir\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Training\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# Evaluation (Now includes evaluation on the test set)\n",
        "evaluator = COCOEvaluator(\"my_dataset_valid\", output_dir=cfg.OUTPUT_DIR) # Evaluate on validation set first\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_valid\")\n",
        "print(\"Evaluation on Validation Set:\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
        "\n",
        "evaluator = COCOEvaluator(\"my_dataset_test\", output_dir=cfg.OUTPUT_DIR) # Then evaluate on test set\n",
        "test_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
        "print(\"Evaluation on Test Set:\")\n",
        "print(inference_on_dataset(trainer.model, test_loader, evaluator))\n",
        "\n",
        "\n",
        "# Inference (Example - same as before)\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "im = cv2.imread(os.path.join(data_dir, \"test/path/to/your/test_image.jpg\")) # Example: reading from the test directory\n",
        "outputs = predictor(im)\n",
        "v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(torch.device(\"cpu\")))\n",
        "\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e383fca",
      "metadata": {
        "id": "0e383fca"
      },
      "outputs": [],
      "source": [
        "# Inference (Example - same as before)\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "im = cv2.imread(os.path.join(data_dir, \"\")) # Example: reading from the test directory\n",
        "outputs = predictor(im)\n",
        "v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(torch.device(\"cpu\")))\n",
        "\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec7b8ab",
      "metadata": {
        "id": "0ec7b8ab"
      },
      "outputs": [],
      "source": [
        "# ... (other code)\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# *** REPLACE THIS WITH THE ACTUAL PATH TO YOUR TEST IMAGE ***\n",
        "image_path = \"/content/drive/MyDrive/My First Project.v1i.coco-segmentation/test/frame_1320_jpg.rf.4aee7ba0b6ca9bc34b30f0dcc2559dcf.jpg\"  # Example - CHANGE THIS!\n",
        "# OR\n",
        "# image_path = os.path.join(data_dir, \"test\", \"frame_1320_jpg.rf.4aee7ba0b6ca9bc34b30f0dcc2559dcf.jpg\")  # Example - CHANGE THIS!\n",
        "\n",
        "print(f\"Trying to read image at: {image_path}\") # Print the path for debugging\n",
        "im = cv2.imread(image_path)\n",
        "\n",
        "if im is None:\n",
        "    print(f\"ERROR: Could not read image at: {image_path}\")  # Handle the error\n",
        "else:\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(torch.device(\"cpu\")))\n",
        "\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7421a5e2-ea51-465e-8078-f684b3a70bfc",
      "metadata": {
        "collapsed": true,
        "id": "7421a5e2-ea51-465e-8078-f684b3a70bfc",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python torch torchvision supervision transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd55c1d7-fa8c-4fd2-81ef-b62782c0706a",
      "metadata": {
        "id": "bd55c1d7-fa8c-4fd2-81ef-b62782c0706a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import PIL\n",
        "\n",
        "def extract_frames(video_path, interval=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_count % interval == 0:\n",
        "            # Convert BGR to RGB immediately\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame_rgb)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067c8848-4683-475c-8775-8a58844c94f1",
      "metadata": {
        "collapsed": true,
        "id": "067c8848-4683-475c-8775-8a58844c94f1",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import supervision as sv\n",
        "import numpy as np\n",
        "\n",
        "depth_estimator = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-large-hf\")\n",
        "segmenter = pipeline(\"image-segmentation\", model=\"facebook/mask2former-swin-large-coco-panoptic\")\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from transformers import Mask2FormerImageProcessor\n",
        "\n",
        "# Initialize processor for proper mask decoding\n",
        "processor = Mask2FormerImageProcessor()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "op-1jCmIExmd",
      "metadata": {
        "id": "op-1jCmIExmd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from transformers import pipeline\n",
        "\n",
        "def process_frame_with_roi(frame,roi):\n",
        "    \"\"\"\n",
        "    Processes a frame with a pre-defined ROI, calculating depth and creating a mask.\n",
        "\n",
        "    Args:\n",
        "        frame: The image frame (NumPy array).\n",
        "        roi_coordinates: A list or tuple of (x1, y1, x2, y2) coordinates\n",
        "                         representing the top-left and bottom-right corners of the ROI.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the depth map and a list containing the ROI mask (as a \"wagon_mask\").\n",
        "        Returns (None, None) if ROI coordinates are invalid.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    height, width, _ = frame.shape\n",
        "\n",
        "    # Validate ROI coordinates (same as before) (461, 3, 1846, 1937)\n",
        "    x1, y1, x2, y2 = roi\n",
        "    if not (0 <= x1 < width and 0 <= y1 < height and 0 <= x2 < width and 0 <= y2 < height and x1 < x2 and y1 < y2):\n",
        "        print(\"Invalid ROI coordinates.\")\n",
        "        return None, None\n",
        "\n",
        "    # 1. Depth Estimation (same as before)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(frame_rgb)\n",
        "    depth_result = depth_estimator(pil_image)\n",
        "    depth_map = np.array(depth_result[\"depth\"])\n",
        "\n",
        "    # 2. Create ROI Mask (same as before)\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    mask[y1:y2, x1:x2] = 255\n",
        "\n",
        "    # 3. Prepare the wagon_masks list (containing just the ROI mask)\n",
        "    wagon_masks = [mask] # Put the mask in a list, just like the segmentation version\n",
        "    return depth_map, wagon_masks , depth_result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6abd49e4-4677-41fd-a5ba-fd06aef550b1",
      "metadata": {
        "id": "6abd49e4-4677-41fd-a5ba-fd06aef550b1"
      },
      "outputs": [],
      "source": [
        "def calibrate_depth(wagon_mask, depth_map, reference_length_pixels, reference_real_length):\n",
        "    \"\"\"\n",
        "    Calculate depth scaling factor using known reference dimension\n",
        "    (e.g., wagon width or height in real world)\n",
        "    \"\"\"\n",
        "    # Get pixel coordinates of reference dimension\n",
        "    y, x = np.where(wagon_mask)\n",
        "    pixel_length = max(x.max() - x.min(), y.max() - y.min())\n",
        "    print(f\"Pixel length: {pixel_length}\")\n",
        "    # Calculate scale factor (meters per pixel)\n",
        "    scale_factor = reference_real_length / pixel_length\n",
        "    return scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227ea3c0-1d29-4821-8603-d9c1d7dc449b",
      "metadata": {
        "id": "227ea3c0-1d29-4821-8603-d9c1d7dc449b"
      },
      "outputs": [],
      "source": [
        "def calculate_volume(depth_map, mask, scale_factor):\n",
        "    masked_depth = depth_map * mask\n",
        "    material_height = masked_depth.max() - masked_depth.min()\n",
        "\n",
        "    # Calculate area in real-world units\n",
        "    pixel_area = np.sum(mask)\n",
        "    real_area = pixel_area * (scale_factor ** 2)\n",
        "\n",
        "    volume = real_area * material_height * scale_factor\n",
        "    return volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70d13a6-df7d-40d3-8382-42804755e187",
      "metadata": {
        "id": "b70d13a6-df7d-40d3-8382-42804755e187"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def process_video(video_path,roi,track_id):\n",
        "\n",
        "    frames = extract_frames(video_path)\n",
        "\n",
        "    wagon_volumes = defaultdict(list)\n",
        "\n",
        "    for frame in frames:\n",
        "        depth_map, wagon_masks = process_frame_with_roi(frame,roi)\n",
        "\n",
        "        for idx, mask in enumerate(wagon_masks):\n",
        "            # Implement tracking ID (could use ByteTrack or simple centroid tracking)\n",
        "             # Implement tracking logic\n",
        "\n",
        "            # Get reference dimension (from known wagon specs)\n",
        "            scale = calibrate_depth(mask, depth_map,\n",
        "                                  reference_length_pixels=150,\n",
        "                                  reference_real_length=2.5)  # 2.5m reference\n",
        "\n",
        "            volume = calculate_volume(depth_map, mask, scale)\n",
        "            wagon_volumes[track_id].append(volume)\n",
        "\n",
        "    # Average volumes across frames\n",
        "    final_volumes = {tid: np.mean(vals) for tid, vals in wagon_volumes.items()}\n",
        "    return final_volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9WzoZLSCnzv",
      "metadata": {
        "id": "d9WzoZLSCnzv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6365f90-3dab-4683-847c-00962471c3d0",
      "metadata": {
        "id": "a6365f90-3dab-4683-847c-00962471c3d0"
      },
      "outputs": [],
      "source": [
        "video_path = r\"/content/drive/MyDrive/2 copy.mp4\"\n",
        "track_id = 1\n",
        "roi = (461, 3, 1846, 1937)\n",
        "# Execute the pipeline\n",
        "final_volumes = process_video(video_path,roi,track_id)\n",
        "\n",
        "# Print results\n",
        "print(\"Wagon Volumes (cubic meters):\")\n",
        "for wagon_id, volume in final_volumes.items():\n",
        "    print(f\"Wagon {wagon_id}: {volume:.2f} m³\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eKQonkVy9PUV",
      "metadata": {
        "id": "eKQonkVy9PUV"
      },
      "outputs": [],
      "source": [
        "depth_map, wagon_masks , depth_result = process_frame_with_roi(cv2.imread(\"/content/Screenshot 2025-02-01 at 11.09.23.png\"),roi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EUterWAPCuOj",
      "metadata": {
        "id": "EUterWAPCuOj"
      },
      "outputs": [],
      "source": [
        "print(depth_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Egb6uENUFEM8",
      "metadata": {
        "id": "Egb6uENUFEM8"
      },
      "outputs": [],
      "source": [
        "depth_result[\"depth\"].show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zZeKsOQ5FL4g",
      "metadata": {
        "id": "zZeKsOQ5FL4g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "depth_image = np.array(depth_result[\"depth\"])  # Convert PIL image to NumPy array\n",
        "\n",
        "plt.imshow(depth_image, cmap=\"magma\")  # Change cmap to 'gray' or 'viridis' if needed\n",
        "plt.colorbar()\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}