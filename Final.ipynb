{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e742853c",
      "metadata": {
        "id": "e742853c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, models\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "23b17c8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23b17c8d",
        "outputId": "95605d6f-642e-44b1-8b30-672dfbafd728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1c687b9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1c687b9f",
        "outputId": "de37733c-d6e3-40d8-d153-f211cebc6f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Directory contents: ['.config', 'drive', 'sample_data']\n",
            "Classes in directory: ['LOADED', 'UNLOADED']\n",
            "Number of images in LOADED: 51\n",
            "Number of images in UNLOADED: 0\n",
            "Found 41 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Number of training samples: 41\n",
            "Number of validation samples: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m44,302,848\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">44,302,848</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,396,609\u001b[0m (169.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,396,609</span> (169.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,396,609\u001b[0m (169.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,396,609</span> (169.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13s/step - accuracy: 0.5203 - loss: 0.3611 - val_accuracy: 1.0000 - val_loss: 1.4326e-13\n",
            "Epoch 2/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.3878e-12 - val_accuracy: 1.0000 - val_loss: 1.0342e-24\n",
            "Epoch 3/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 942ms/step - accuracy: 1.0000 - loss: 5.7554e-22 - val_accuracy: 1.0000 - val_loss: 1.7672e-33\n",
            "Epoch 4/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 7.1341e-31 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 2.4237e-40 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.2752e-43 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 742ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850ms/step\n",
            "Prediction for /content/drive/MyDrive/PhotosOfLoaded/DATASET/LOADED/1.jpg: UnLoaded\n",
            "\n",
            "Final Training Metrics:\n",
            "Training Accuracy: 1.0000\n",
            "Training Loss: 0.0000\n",
            "Validation Accuracy: 1.0000\n",
            "Validation Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import os\n",
        "\n",
        "# Debug prints to check directory and contents\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Directory contents:\", os.listdir())\n",
        "\n",
        "# Set image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Set up data generators with image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Specify the dataset directory\n",
        "dataset_dir = '/content/drive/MyDrive/PhotosOfLoaded/DATASET'  # Assuming the damaged and no_damage folders are in the current directory\n",
        "\n",
        "\n",
        "if not os.path.exists(dataset_dir):\n",
        "    raise ValueError(f\"Dataset directory not found: {dataset_dir}\")\n",
        "\n",
        "\n",
        "# Debug prints for directory structure\n",
        "print(\"Classes in directory:\", os.listdir(dataset_dir))\n",
        "\n",
        "# Count images in each folder\n",
        "for folder in ['LOADED', 'UNLOADED']:\n",
        "    folder_path = os.path.join(dataset_dir, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        print(f\"Number of images in {folder}: {len(files)}\")\n",
        "    else:\n",
        "        print(f\"Folder {folder} does not exist!\")\n",
        "\n",
        "# Create train and validation generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    classes=['LOADED', 'UNLOADED']\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    classes=['LOADED', 'UNLOADED']\n",
        ")\n",
        "\n",
        "# Print the number of samples\n",
        "print(\"Number of training samples:\", train_generator.samples)\n",
        "print(\"Number of validation samples:\", validation_generator.samples)\n",
        "\n",
        "# Create the CNN model\n",
        "model = Sequential([\n",
        "    # First Convolutional Block\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Flatten and Dense Layers\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('damage_detection_model.h5')\n",
        "\n",
        "# Function to predict a single image\n",
        "def predict_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(\n",
        "        image_path,\n",
        "        target_size=(img_width, img_height)\n",
        "    )\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    img_array /= 255.\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    return \"Loaded\" if prediction[0] > 0.5 else \"UnLoaded\"\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Example of how to use the prediction function\n",
        "# Replace 'path_to_test_image.jpg' with an actual image path\n",
        "try:\n",
        "    test_image_path = '/content/drive/MyDrive/PhotosOfLoaded/DATASET/LOADED/1.jpg'  # Replace with your test image path\n",
        "    result = predict_image(test_image_path)\n",
        "    print(f\"Prediction for {test_image_path}: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during prediction: {str(e)}\")\n",
        "\n",
        "# Print final metrics\n",
        "final_train_accuracy = history.history['accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_accuracy = history.history['val_accuracy'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(\"\\nFinal Training Metrics:\")\n",
        "print(f\"Training Accuracy: {final_train_accuracy:.4f}\")\n",
        "print(f\"Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {final_val_accuracy:.4f}\")\n",
        "print(f\"Validation Loss: {final_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b096908f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b096908f",
        "outputId": "397270e1-221b-43a2-f45e-02631962a0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Prediction for /content/drive/MyDrive/PhotosOfLoaded/DATASET/LOADED/1.jpg: Loaded\n"
          ]
        }
      ],
      "source": [
        "# Function to predict a single image\n",
        "def predict_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(\n",
        "        image_path,\n",
        "        target_size=(img_width, img_height)\n",
        "    )\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    img_array /= 255.\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    return \"Un Loaded\" if prediction[0] > 0.5 else \"Loaded\"\n",
        "\n",
        "try:\n",
        "    test_image_path = '/content/drive/MyDrive/PhotosOfLoaded/DATASET/LOADED/1.jpg'  # Replace with your test image path\n",
        "    result = predict_image(test_image_path)\n",
        "    print(f\"Prediction for {test_image_path}: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during prediction: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2ec214de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ec214de",
        "outputId": "3cb6e96e-4389-4b4e-e225-0d45d2d4f098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch + Detectron2 (Colab 2025 compatible)\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[1]\n",
        "print(f\"Torch: {TORCH_VERSION} +cu{CUDA_VERSION}\")\n",
        "\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu121/torch2.4/index.html\n",
        "# Restart runtime after this cell runs"
      ],
      "metadata": {
        "id": "e7VsJsX02wwk"
      },
      "id": "e7VsJsX02wwk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "print(\"✅ Detectron2 ready!\")"
      ],
      "metadata": {
        "id": "CylEn8jG3Cmu"
      },
      "id": "CylEn8jG3Cmu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "28b28267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "28b28267",
        "outputId": "27c11dc8-8c99-4a26-c927-27615345bf82"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'detectron2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4080084990.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDefaultTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "import cv2\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2 import model_zoo\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Data and model paths (adjust if needed)\n",
        "data_dir = \"/content/drive/MyDrive/Wagon edges detection.v1i.coco\"\n",
        "output_dir = \"/content/drive/MyDrive/Wagon_count_output\"\n",
        "for d in [\"train\",\"test\"]:\n",
        "    json_file = os.path.join(data_dir, d, \"_annotations.coco.json\")\n",
        "    image_dir = os.path.join(data_dir, d)\n",
        "    register_coco_instances(f\"my_dataset_{d}\", {}, json_file, image_dir)\n",
        "\n",
        "# Metadata\n",
        "metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "# metadata.thing_classes = [\"objects\", \"Damage\", \"Damege\", \"Debris\", \"Obstacle\"]\n",
        "# metadata.thing_dataset_id_to_contiguous_id = {0:0, 1:1, 2:2, 3:3, 4:4}\n",
        "metadata.thing_classes = [\"Rail-Wagon\", \"Front\", \"Rear\"]\n",
        "metadata.thing_dataset_id_to_contiguous_id = {0: 0, 1: 1, 2: 2}\n",
        "\n",
        "\n",
        "# Detectron2 Configuration\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "# OR, if you have a local config file:\n",
        "#cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.VAL = ()\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\",)  # Include the test set\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2  # Adjust based on GPU\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000  # Adjust as needed\n",
        "cfg.OUTPUT_DIR = output_dir\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ... (other code)\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# *** REPLACE THIS WITH THE ACTUAL PATH TO YOUR TEST IMAGE ***\n",
        "image_path = \"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_1250_jpg.rf.d06658a856f4f86b79c7fdceb4cd7566.jpg\"\n",
        "# OR\n",
        "# image_path = os.path.join(data_dir, \"test\", \"frame_1320_jpg.rf.4aee7ba0b6ca9bc34b30f0dcc2559dcf.jpg\")  # Example - CHANGE THIS!\n",
        "\n",
        "print(f\"Trying to read image at: {image_path}\") # Print the path for debugging\n",
        "im = cv2.imread(image_path)\n",
        "\n",
        "if im is None:\n",
        "    print(f\"ERROR: Could not read image at: {image_path}\")  # Handle the error\n",
        "else:\n",
        "    outputs = predictor(im)\n",
        "    instances = outputs[\"instances\"].to(torch.device(\"cpu\"))\n",
        "    high_conf_instances = instances[instances.scores > 0.8]\n",
        "\n",
        "    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "    out = v.draw_instance_predictions(high_conf_instances)\n",
        "\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506e9b16",
      "metadata": {
        "id": "506e9b16"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # Load your trained model\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "paths=[\"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_1250_jpg.rf.d06658a856f4f86b79c7fdceb4cd7566.jpg\",\"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_1480_jpg.rf.f2059211ffa29fac69ce6d52934dbe9d.jpg\",\"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_1970_jpg.rf.473aed8e1ed837640b689179aed9eb75.jpg\",\"/content/drive/MyDrive/Wagon edges detection.v1i.coco/test/frame_550_jpg.rf.e9a4990d405cfc21dfa87a3cdafaf424.jpg\"]\n",
        "\n",
        "\n",
        "for i in range(len(paths)):\n",
        "    frame = cv2.imread(paths[i])\n",
        "\n",
        "\n",
        "    outputs = predictor(frame)\n",
        "\n",
        "    # Filter by confidence threshold\n",
        "    instances = outputs[\"instances\"].to(torch.device(\"cpu\"))\n",
        "    high_conf_instances = instances[instances.scores > 0.8]\n",
        "\n",
        "    v = Visualizer(frame[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "    out = v.draw_instance_predictions(high_conf_instances)\n",
        "\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])  # Display the frame in Colab\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
        "        break\n",
        "\n",
        "\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18665be9-800e-4440-86d3-a07d4cdaad90",
      "metadata": {
        "id": "18665be9-800e-4440-86d3-a07d4cdaad90"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ebf5cd0-ec03-40bb-a21a-9b3ec3e8fa63",
      "metadata": {
        "id": "5ebf5cd0-ec03-40bb-a21a-9b3ec3e8fa63"
      },
      "outputs": [],
      "source": [
        "# Import additional required libraries\n",
        "import tensorflow as tf\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import utils\n",
        "from mrcnn.config import Config\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Define Mask R-CNN Configuration\n",
        "class CustomConfig(Config):\n",
        "    NAME = \"custom\"\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1  # Background + your class\n",
        "\n",
        "    # Number of training steps per epoch\n",
        "    STEPS_PER_EPOCH = 100\n",
        "\n",
        "    # Learning rate\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    # Skip detections with < 90% confidence\n",
        "    DETECTION_MIN_CONFIDENCE = 0.9\n",
        "\n",
        "    # Setting batch size\n",
        "    IMAGES_PER_GPU = 1\n",
        "    GPU_COUNT = 1\n",
        "\n",
        "# Function to prepare dataset\n",
        "def prepare_dataset(df, BASE_PATH, target_size=(1024, 1024)):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Load image\n",
        "        img_path = os.path.join(BASE_PATH, str(row['filename']))\n",
        "        if os.path.exists(img_path):\n",
        "            # Read and preprocess image\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = cv2.resize(image, target_size)\n",
        "\n",
        "            # Create mask (you'll need to modify this based on your annotations)\n",
        "            mask = np.zeros(target_size + (1,), dtype=np.bool)\n",
        "            # Add mask creation logic here based on your annotations\n",
        "\n",
        "            images.append(image)\n",
        "            masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Training function\n",
        "def train_model(train_images, train_masks, val_images, val_masks, config):\n",
        "    # Initialize model\n",
        "    model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                             model_dir=\"./logs/\")\n",
        "\n",
        "    # Load pre-trained COCO weights\n",
        "    model.load_weights('mask_rcnn_coco.h5', by_name=True,\n",
        "                      exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n",
        "    # Train the model\n",
        "    model.train(train_images, train_masks,\n",
        "                val_images, val_masks,\n",
        "                learning_rate=config.LEARNING_RATE,\n",
        "                epochs=30,\n",
        "                layers='heads')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Prediction function\n",
        "def predict_image(image_path, model, config):\n",
        "    # Read and preprocess image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detect objects\n",
        "    results = model.detect([image], verbose=1)\n",
        "    r = results[0]\n",
        "\n",
        "    return r\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize configuration\n",
        "    config = CustomConfig()\n",
        "    config.display()\n",
        "\n",
        "    # Read your dataset\n",
        "    df = pd.read_csv('/Users/bhavya/Desktop/allnewphotos/image_labels.csv')\n",
        "    BASE_PATH = \"/Users/bhavya/Desktop/allnewphotos/\"\n",
        "\n",
        "    # Prepare dataset\n",
        "    images, masks = prepare_dataset(df, BASE_PATH)\n",
        "\n",
        "    # Split dataset\n",
        "    train_idx, val_idx = train_test_split(range(len(images)), test_size=0.2)\n",
        "    train_images, train_masks = images[train_idx], masks[train_idx]\n",
        "    val_images, val_masks = images[val_idx], masks[val_idx]\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(train_images, train_masks, val_images, val_masks, config)\n",
        "\n",
        "    # Test predictions\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i, img_path in enumerate(df['filename'].head(5)):\n",
        "        full_path = os.path.join(BASE_PATH, img_path)\n",
        "        results = predict_image(full_path, model, config)\n",
        "\n",
        "        # Display results\n",
        "        image = cv2.imread(full_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f'Detection {i+1}')\n",
        "\n",
        "        # Draw masks\n",
        "        for j in range(len(results['masks'])):\n",
        "            mask = results['masks'][:, :, j]\n",
        "            plt.imshow(mask, alpha=0.5)\n",
        "\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63158c3-d32c-47c1-a124-d26f24bdda6b",
      "metadata": {
        "id": "f63158c3-d32c-47c1-a124-d26f24bdda6b"
      },
      "outputs": [],
      "source": [
        "!conda install -c conda-forge keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1997366b-f80e-4ab6-8fe5-d2398f9b9219",
      "metadata": {
        "id": "1997366b-f80e-4ab6-8fe5-d2398f9b9219"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import os\n",
        "\n",
        "# Debug prints to check directory and contents\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Directory contents:\", os.listdir())\n",
        "\n",
        "# Set image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Set up data generators with image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Specify the dataset directory\n",
        "dataset_dir = '/Users/bhavya/Desktop/Wagonframes'  # Assuming the damaged and no_damage folders are in the current directory\n",
        "\n",
        "# Debug prints for directory structure\n",
        "print(\"Classes in directory:\", os.listdir(dataset_dir))\n",
        "\n",
        "# Count images in each folder\n",
        "for folder in ['damaged', 'no_damage']:\n",
        "    folder_path = os.path.join(dataset_dir, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        print(f\"Number of images in {folder}: {len(files)}\")\n",
        "    else:\n",
        "        print(f\"Folder {folder} does not exist!\")\n",
        "\n",
        "# Create train and validation generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    classes=['damaged', 'no_damage']\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    classes=['damaged', 'no_damage']\n",
        ")\n",
        "\n",
        "# Print the number of samples\n",
        "print(\"Number of training samples:\", train_generator.samples)\n",
        "print(\"Number of validation samples:\", validation_generator.samples)\n",
        "\n",
        "# Create the CNN model\n",
        "model = Sequential([\n",
        "    # First Convolutional Block\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Flatten and Dense Layers\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('damage_detection_model.h5')\n",
        "\n",
        "# Function to predict a single image\n",
        "def predict_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(\n",
        "        image_path,\n",
        "        target_size=(img_width, img_height)\n",
        "    )\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    img_array /= 255.\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    return \"Damaged\" if prediction[0] > 0.5 else \"No Damage\"\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Example of how to use the prediction function\n",
        "# Replace 'path_to_test_image.jpg' with an actual image path\n",
        "try:\n",
        "    test_image_path = '/Users/bhavya/Desktop/testing.png'  # Replace with your test image path\n",
        "    result = predict_image(test_image_path)\n",
        "    print(f\"Prediction for {test_image_path}: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during prediction: {str(e)}\")\n",
        "\n",
        "# Print final metrics\n",
        "final_train_accuracy = history.history['accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_accuracy = history.history['val_accuracy'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(\"\\nFinal Training Metrics:\")\n",
        "print(f\"Training Accuracy: {final_train_accuracy:.4f}\")\n",
        "print(f\"Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {final_val_accuracy:.4f}\")\n",
        "print(f\"Validation Loss: {final_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e5291a-75b8-461b-8088-c9fff4c81db2",
      "metadata": {
        "id": "56e5291a-75b8-461b-8088-c9fff4c81db2"
      },
      "outputs": [],
      "source": [
        "result = predict_image('/Users/bhavya/Desktop/testing.png')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66abd511-b71a-477a-8199-d87c50c71609",
      "metadata": {
        "id": "66abd511-b71a-477a-8199-d87c50c71609"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import time\n",
        "import logging\n",
        "\n",
        "class WagonDetector:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize background subtractor\n",
        "        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(\n",
        "            history=self.config.history,\n",
        "            varThreshold=self.config.var_threshold,\n",
        "            detectShadows=self.config.detect_shadows\n",
        "        )\n",
        "\n",
        "        # Initialize counters and trackers\n",
        "        self.wagon_count = 0\n",
        "        self.crossed_objects = set()\n",
        "        self.object_tracker = deque(maxlen=self.config.tracker_length)\n",
        "        self.last_count_frame = 0\n",
        "\n",
        "        # Initialize ROI\n",
        "        self.roi_points = None\n",
        "        self.roi_mask = None\n",
        "\n",
        "        # Setup logging\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        self.logger = logging.getLogger('WagonDetector')\n",
        "\n",
        "    def setup_roi(self, frame):\n",
        "        \"\"\"Initialize ROI based on frame dimensions with offset\"\"\"\n",
        "        height, width = frame.shape[:2]\n",
        "\n",
        "        # Define offset\n",
        "        x_offset = 400  # pixels from left\n",
        "        y_offset = 400  # pixels from top\n",
        "\n",
        "        # Define ROI points (trapezoid shape) with offset\n",
        "        self.roi_points = np.array([\n",
        "            [int(width * 0.1) + x_offset, int(height * 0.4) + y_offset],  # Top left\n",
        "            [int(width * 0.9) + x_offset, int(height * 0.4) + y_offset],  # Top right\n",
        "            [int(width * 0.95) + x_offset, int(height * 0.8) + y_offset], # Bottom right\n",
        "            [int(width * 0.05) + x_offset, int(height * 0.8) + y_offset]  # Bottom left\n",
        "        ], np.int32)\n",
        "\n",
        "        # Ensure ROI points are within frame boundaries\n",
        "        self.roi_points = np.clip(self.roi_points, [0, 0], [width-1, height-1])\n",
        "\n",
        "        # Create ROI mask\n",
        "        self.roi_mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
        "        cv2.fillPoly(self.roi_mask, [self.roi_points], 255)\n",
        "\n",
        "    def preprocess_frame(self, frame):\n",
        "        \"\"\"Preprocess frame for detection\"\"\"\n",
        "        # Resize frame\n",
        "        frame = cv2.resize(frame, (self.config.resize_width, self.config.resize_height))\n",
        "\n",
        "        # Initialize ROI if not already done\n",
        "        if self.roi_mask is None:\n",
        "            self.setup_roi(frame)\n",
        "\n",
        "        # Apply ROI mask\n",
        "        roi_frame = cv2.bitwise_and(frame, frame, mask=self.roi_mask)\n",
        "\n",
        "        # Apply Gaussian blur\n",
        "        blurred = cv2.GaussianBlur(roi_frame,\n",
        "                                  (self.config.blur_kernel_size, self.config.blur_kernel_size),\n",
        "                                  0)\n",
        "\n",
        "        return blurred\n",
        "\n",
        "    def detect_motion(self, frame):\n",
        "        \"\"\"Detect motion using background subtraction\"\"\"\n",
        "        # Apply background subtraction\n",
        "        fg_mask = self.bg_subtractor.apply(frame)\n",
        "\n",
        "        # Apply morphological operations\n",
        "        kernel = np.ones((5,5), np.uint8)\n",
        "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
        "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "        return fg_mask\n",
        "\n",
        "    def process_frame(self, frame, frame_count):\n",
        "        \"\"\"Process a single frame\"\"\"\n",
        "        # Preprocess frame\n",
        "        processed = self.preprocess_frame(frame)\n",
        "        height, width = processed.shape[:2]\n",
        "\n",
        "        # Detect motion\n",
        "        motion_mask = self.detect_motion(processed)\n",
        "\n",
        "        # Find contours\n",
        "        contours, _ = cv2.findContours(\n",
        "            motion_mask,\n",
        "            cv2.RETR_EXTERNAL,\n",
        "            cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        # Draw ROI\n",
        "        cv2.polylines(frame, [self.roi_points], True, (0, 255, 255), 2)\n",
        "\n",
        "        # Draw counting line with offset\n",
        "        x_offset = 400\n",
        "        y_offset = 400\n",
        "        line_y = int(height * self.config.line_position) + y_offset\n",
        "        cv2.line(frame,\n",
        "                 (int(width * 0.1) + x_offset, line_y),\n",
        "                 (int(width * 0.9) + x_offset, line_y),\n",
        "                 (0, 255, 0), 2)\n",
        "\n",
        "        # Process contours\n",
        "        for contour in contours:\n",
        "            area = cv2.contourArea(contour)\n",
        "\n",
        "            if area > self.config.min_area:\n",
        "                # Get contour centroid\n",
        "                M = cv2.moments(contour)\n",
        "                if M[\"m00\"] != 0:\n",
        "                    cx = int(M[\"m10\"] / M[\"m00\"])\n",
        "                    cy = int(M[\"m01\"] / M[\"m00\"])\n",
        "\n",
        "                    # Check if centroid is in ROI\n",
        "                    if cv2.pointPolygonTest(self.roi_points, (cx, cy), False) >= 0:\n",
        "                        # Draw centroid\n",
        "                        cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)\n",
        "\n",
        "                        # Check if object crosses the counting line\n",
        "                        if (abs(cy - line_y) < self.config.line_threshold and\n",
        "                            frame_count - self.last_count_frame > self.config.min_frames_between_counts):\n",
        "\n",
        "                            object_id = f\"{cx}_{cy}\"\n",
        "                            if object_id not in self.crossed_objects:\n",
        "                                self.wagon_count += 1\n",
        "                                self.crossed_objects.add(object_id)\n",
        "                                self.last_count_frame = frame_count\n",
        "                                self.logger.info(f\"Wagon detected! Count: {self.wagon_count}\")\n",
        "\n",
        "                        # Draw bounding rectangle\n",
        "                        x, y, w, h = cv2.boundingRect(contour)\n",
        "                        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # Add text overlay\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            f\"Wagon Count: {self.wagon_count}\",\n",
        "            (10, 30),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            1,\n",
        "            (0, 255, 0),\n",
        "            2\n",
        "        )\n",
        "\n",
        "        return frame\n",
        "\n",
        "class WagonDetectorConfig:\n",
        "    def __init__(self):\n",
        "        # Video processing parameters\n",
        "        self.resize_width = 1920  # Increased for better resolution\n",
        "        self.resize_height = 1080\n",
        "        self.blur_kernel_size = 21\n",
        "\n",
        "        # Background subtractor parameters\n",
        "        self.history = 500\n",
        "        self.var_threshold = 16\n",
        "        self.detect_shadows = False\n",
        "\n",
        "        # Detection parameters\n",
        "        self.min_area = 3000\n",
        "        self.line_position = 0.4  # Adjusted for offset\n",
        "        self.line_threshold = 10\n",
        "        self.min_frames_between_counts = 15\n",
        "        self.tracker_length = 50\n",
        "\n",
        "def process_video(video_path, output_path=None, display=True):\n",
        "    # Create configuration\n",
        "    config = WagonDetectorConfig()\n",
        "\n",
        "    # Initialize detector\n",
        "    detector = WagonDetector(config)\n",
        "\n",
        "    # Open video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(\"Error opening video file\")\n",
        "\n",
        "    # Get video properties\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Initialize video writer if output path is provided\n",
        "    if output_path:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    processing_times = []\n",
        "\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Process frame\n",
        "            processed_frame = detector.process_frame(frame, frame_count)\n",
        "\n",
        "            # Calculate processing time\n",
        "            processing_time = time.time() - start_time\n",
        "            processing_times.append(processing_time)\n",
        "\n",
        "            # Write frame if output path is provided\n",
        "            if output_path:\n",
        "                out.write(processed_frame)\n",
        "\n",
        "            # Display frame\n",
        "            if display:\n",
        "                cv2.imshow('Wagon Detection', processed_frame)\n",
        "\n",
        "                # Break loop if 'q' is pressed\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing video: {str(e)}\")\n",
        "\n",
        "    finally:\n",
        "        # Calculate and print performance metrics\n",
        "        if processing_times:\n",
        "            avg_processing_time = np.mean(processing_times)\n",
        "            avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n",
        "\n",
        "            print(f\"\\nPerformance Metrics:\")\n",
        "            print(f\"Average processing time per frame: {avg_processing_time:.3f} seconds\")\n",
        "            print(f\"Average FPS: {avg_fps:.2f}\")\n",
        "\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        if output_path:\n",
        "            out.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    return detector.wagon_count\n",
        "\n",
        "def main():\n",
        "    # Example usage with specific video dimensions\n",
        "    video_path = \"3.mp4\"  # Replace with your video path\n",
        "    output_path = \"output_video.mp4\"     # Optional output path\n",
        "\n",
        "    try:\n",
        "        print(\"Processing video...\")\n",
        "        wagon_count = process_video(video_path, output_path)\n",
        "        print(f\"\\nTotal wagons detected: {wagon_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dbc93e5",
      "metadata": {
        "collapsed": true,
        "id": "8dbc93e5",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install pycocotools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18610902",
      "metadata": {
        "id": "18610902"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd02667",
      "metadata": {
        "id": "bcd02667"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/My Drive/Wagon frames/train copy\"\n",
        "coco_json = f\"{dataset_path}/_annotations.coco.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0c6a70",
      "metadata": {
        "collapsed": true,
        "id": "2e0c6a70",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Register dataset\n",
        "register_coco_instances(\"wagon_train\", {}, coco_json, dataset_path)\n",
        "\n",
        "# Verify dataset\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "metadata = MetadataCatalog.get(\"wagon_train\")\n",
        "dataset_dicts = DatasetCatalog.get(\"wagon_train\")\n",
        "\n",
        "# Show sample images\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    img_path = d[\"file_name\"]\n",
        "    img = cv2.imread(img_path)\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "    out = v.draw_dataset_dict(d)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(out.get_image())\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89469169",
      "metadata": {
        "id": "89469169"
      },
      "outputs": [],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f97e9fc",
      "metadata": {
        "id": "4f97e9fc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "import cv2\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2 import model_zoo\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Data and model paths (adjust if needed)\n",
        "data_dir = \"/content/drive/MyDrive/My First Project.v1i.coco-segmentation\"\n",
        "output_dir = \"/content/drive/MyDrive/detectron2_output\"\n",
        "\n",
        "# Dataset Registration (Now includes \"test\")\n",
        "for d in [\"train\", \"valid\", \"test\"]:\n",
        "    json_file = os.path.join(data_dir, d, \"_annotations.coco.json\")\n",
        "    image_dir = os.path.join(data_dir, d)\n",
        "    register_coco_instances(f\"my_dataset_{d}\", {}, json_file, image_dir)\n",
        "\n",
        "# Metadata\n",
        "metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "metadata.thing_classes = [\"objects\", \"Damage\", \"Damege\", \"Debris\", \"Obstacle\"]\n",
        "metadata.thing_dataset_id_to_contiguous_id = {0:0, 1:1, 2:2, 3:3, 4:4}\n",
        "\n",
        "# Detectron2 Configuration\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "# OR, if you have a local config file:\n",
        "#cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.VAL = (\"my_dataset_valid\",)\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\",)  # Include the test set\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2  # Adjust based on GPU\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000  # Adjust as needed\n",
        "cfg.OUTPUT_DIR = output_dir\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Training\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# Evaluation (Now includes evaluation on the test set)\n",
        "evaluator = COCOEvaluator(\"my_dataset_valid\", output_dir=cfg.OUTPUT_DIR) # Evaluate on validation set first\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_valid\")\n",
        "print(\"Evaluation on Validation Set:\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
        "\n",
        "evaluator = COCOEvaluator(\"my_dataset_test\", output_dir=cfg.OUTPUT_DIR) # Then evaluate on test set\n",
        "test_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
        "print(\"Evaluation on Test Set:\")\n",
        "print(inference_on_dataset(trainer.model, test_loader, evaluator))\n",
        "\n",
        "\n",
        "# Inference (Example - same as before)\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "im = cv2.imread(os.path.join(data_dir, \"test/path/to/your/test_image.jpg\")) # Example: reading from the test directory\n",
        "outputs = predictor(im)\n",
        "v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(torch.device(\"cpu\")))\n",
        "\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e383fca",
      "metadata": {
        "id": "0e383fca"
      },
      "outputs": [],
      "source": [
        "# Inference (Example - same as before)\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "im = cv2.imread(os.path.join(data_dir, \"\")) # Example: reading from the test directory\n",
        "outputs = predictor(im)\n",
        "v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(torch.device(\"cpu\")))\n",
        "\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec7b8ab",
      "metadata": {
        "id": "0ec7b8ab"
      },
      "outputs": [],
      "source": [
        "# ... (other code)\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# *** REPLACE THIS WITH THE ACTUAL PATH TO YOUR TEST IMAGE ***\n",
        "image_path = \"/content/drive/MyDrive/My First Project.v1i.coco-segmentation/test/frame_1320_jpg.rf.4aee7ba0b6ca9bc34b30f0dcc2559dcf.jpg\"  # Example - CHANGE THIS!\n",
        "# OR\n",
        "# image_path = os.path.join(data_dir, \"test\", \"frame_1320_jpg.rf.4aee7ba0b6ca9bc34b30f0dcc2559dcf.jpg\")  # Example - CHANGE THIS!\n",
        "\n",
        "print(f\"Trying to read image at: {image_path}\") # Print the path for debugging\n",
        "im = cv2.imread(image_path)\n",
        "\n",
        "if im is None:\n",
        "    print(f\"ERROR: Could not read image at: {image_path}\")  # Handle the error\n",
        "else:\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(torch.device(\"cpu\")))\n",
        "\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7421a5e2-ea51-465e-8078-f684b3a70bfc",
      "metadata": {
        "collapsed": true,
        "id": "7421a5e2-ea51-465e-8078-f684b3a70bfc",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python torch torchvision supervision transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd55c1d7-fa8c-4fd2-81ef-b62782c0706a",
      "metadata": {
        "id": "bd55c1d7-fa8c-4fd2-81ef-b62782c0706a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import PIL\n",
        "\n",
        "def extract_frames(video_path, interval=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_count % interval == 0:\n",
        "            # Convert BGR to RGB immediately\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame_rgb)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067c8848-4683-475c-8775-8a58844c94f1",
      "metadata": {
        "collapsed": true,
        "id": "067c8848-4683-475c-8775-8a58844c94f1",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import supervision as sv\n",
        "import numpy as np\n",
        "\n",
        "depth_estimator = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-large-hf\")\n",
        "segmenter = pipeline(\"image-segmentation\", model=\"facebook/mask2former-swin-large-coco-panoptic\")\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from transformers import Mask2FormerImageProcessor\n",
        "\n",
        "# Initialize processor for proper mask decoding\n",
        "processor = Mask2FormerImageProcessor()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "op-1jCmIExmd",
      "metadata": {
        "id": "op-1jCmIExmd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from transformers import pipeline\n",
        "\n",
        "def process_frame_with_roi(frame,roi):\n",
        "    \"\"\"\n",
        "    Processes a frame with a pre-defined ROI, calculating depth and creating a mask.\n",
        "\n",
        "    Args:\n",
        "        frame: The image frame (NumPy array).\n",
        "        roi_coordinates: A list or tuple of (x1, y1, x2, y2) coordinates\n",
        "                         representing the top-left and bottom-right corners of the ROI.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the depth map and a list containing the ROI mask (as a \"wagon_mask\").\n",
        "        Returns (None, None) if ROI coordinates are invalid.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    height, width, _ = frame.shape\n",
        "\n",
        "    # Validate ROI coordinates (same as before) (461, 3, 1846, 1937)\n",
        "    x1, y1, x2, y2 = roi\n",
        "    if not (0 <= x1 < width and 0 <= y1 < height and 0 <= x2 < width and 0 <= y2 < height and x1 < x2 and y1 < y2):\n",
        "        print(\"Invalid ROI coordinates.\")\n",
        "        return None, None\n",
        "\n",
        "    # 1. Depth Estimation (same as before)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(frame_rgb)\n",
        "    depth_result = depth_estimator(pil_image)\n",
        "    depth_map = np.array(depth_result[\"depth\"])\n",
        "\n",
        "    # 2. Create ROI Mask (same as before)\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    mask[y1:y2, x1:x2] = 255\n",
        "\n",
        "    # 3. Prepare the wagon_masks list (containing just the ROI mask)\n",
        "    wagon_masks = [mask] # Put the mask in a list, just like the segmentation version\n",
        "    return depth_map, wagon_masks , depth_result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6abd49e4-4677-41fd-a5ba-fd06aef550b1",
      "metadata": {
        "id": "6abd49e4-4677-41fd-a5ba-fd06aef550b1"
      },
      "outputs": [],
      "source": [
        "def calibrate_depth(wagon_mask, depth_map, reference_length_pixels, reference_real_length):\n",
        "    \"\"\"\n",
        "    Calculate depth scaling factor using known reference dimension\n",
        "    (e.g., wagon width or height in real world)\n",
        "    \"\"\"\n",
        "    # Get pixel coordinates of reference dimension\n",
        "    y, x = np.where(wagon_mask)\n",
        "    pixel_length = max(x.max() - x.min(), y.max() - y.min())\n",
        "    print(f\"Pixel length: {pixel_length}\")\n",
        "    # Calculate scale factor (meters per pixel)\n",
        "    scale_factor = reference_real_length / pixel_length\n",
        "    return scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227ea3c0-1d29-4821-8603-d9c1d7dc449b",
      "metadata": {
        "id": "227ea3c0-1d29-4821-8603-d9c1d7dc449b"
      },
      "outputs": [],
      "source": [
        "def calculate_volume(depth_map, mask, scale_factor):\n",
        "    masked_depth = depth_map * mask\n",
        "    material_height = masked_depth.max() - masked_depth.min()\n",
        "\n",
        "    # Calculate area in real-world units\n",
        "    pixel_area = np.sum(mask)\n",
        "    real_area = pixel_area * (scale_factor ** 2)\n",
        "\n",
        "    volume = real_area * material_height * scale_factor\n",
        "    return volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70d13a6-df7d-40d3-8382-42804755e187",
      "metadata": {
        "id": "b70d13a6-df7d-40d3-8382-42804755e187"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def process_video(video_path,roi,track_id):\n",
        "\n",
        "    frames = extract_frames(video_path)\n",
        "\n",
        "    wagon_volumes = defaultdict(list)\n",
        "\n",
        "    for frame in frames:\n",
        "        depth_map, wagon_masks = process_frame_with_roi(frame,roi)\n",
        "\n",
        "        for idx, mask in enumerate(wagon_masks):\n",
        "            # Implement tracking ID (could use ByteTrack or simple centroid tracking)\n",
        "             # Implement tracking logic\n",
        "\n",
        "            # Get reference dimension (from known wagon specs)\n",
        "            scale = calibrate_depth(mask, depth_map,\n",
        "                                  reference_length_pixels=150,\n",
        "                                  reference_real_length=2.5)  # 2.5m reference\n",
        "\n",
        "            volume = calculate_volume(depth_map, mask, scale)\n",
        "            wagon_volumes[track_id].append(volume)\n",
        "\n",
        "    # Average volumes across frames\n",
        "    final_volumes = {tid: np.mean(vals) for tid, vals in wagon_volumes.items()}\n",
        "    return final_volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9WzoZLSCnzv",
      "metadata": {
        "id": "d9WzoZLSCnzv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6365f90-3dab-4683-847c-00962471c3d0",
      "metadata": {
        "id": "a6365f90-3dab-4683-847c-00962471c3d0"
      },
      "outputs": [],
      "source": [
        "video_path = r\"/content/drive/MyDrive/2 copy.mp4\"\n",
        "track_id = 1\n",
        "roi = (461, 3, 1846, 1937)\n",
        "# Execute the pipeline\n",
        "final_volumes = process_video(video_path,roi,track_id)\n",
        "\n",
        "# Print results\n",
        "print(\"Wagon Volumes (cubic meters):\")\n",
        "for wagon_id, volume in final_volumes.items():\n",
        "    print(f\"Wagon {wagon_id}: {volume:.2f} m³\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eKQonkVy9PUV",
      "metadata": {
        "id": "eKQonkVy9PUV"
      },
      "outputs": [],
      "source": [
        "depth_map, wagon_masks , depth_result = process_frame_with_roi(cv2.imread(\"/content/Screenshot 2025-02-01 at 11.09.23.png\"),roi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EUterWAPCuOj",
      "metadata": {
        "id": "EUterWAPCuOj"
      },
      "outputs": [],
      "source": [
        "print(depth_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Egb6uENUFEM8",
      "metadata": {
        "id": "Egb6uENUFEM8"
      },
      "outputs": [],
      "source": [
        "depth_result[\"depth\"].show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zZeKsOQ5FL4g",
      "metadata": {
        "id": "zZeKsOQ5FL4g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "depth_image = np.array(depth_result[\"depth\"])  # Convert PIL image to NumPy array\n",
        "\n",
        "plt.imshow(depth_image, cmap=\"magma\")  # Change cmap to 'gray' or 'viridis' if needed\n",
        "plt.colorbar()\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}